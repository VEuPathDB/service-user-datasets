= User Dataset System

Dataset Store = Minio

== User Dataset Service

=== Actions

[%header, cols="3,1,6"]
|===
| Action | Source | Description

| <<List User Datasets>>
| HTTP
| List all user datasets owned by or shared with a user.

| <<Get User Dataset Info>>
| HTTP
| Get details about a user dataset including who it has been shared with.

| <<Prepare User Dataset Upload>>
| HTTP
|

| <<Upload User Dataset>>
| HTTP
|

| <<Update User Dataset Metadata>>
| HTTP
| Update the user metadata for a user dataset.

| <<Offer User Dataset Share>>
| HTTP
| Offer to share a user dataset.

| Revoke UD Share
| HTTP
| Revoke an outgoing share offer for a user dataset.

| <<Accept User Dataset Share>>
| HTTP
| Accept an incoming share offer for a user dataset.

| Reject UD Share Offer
| HTTP
| Reject an incoming share offer for a user dataset.

| <<Delete User Dataset>>
| HTTP
| Remove a user dataset.

| <<Process Upload>>
| RabbitMQ <1>
| Submit an uploaded user dataset to the appropriate import handler for
  validation and processing.

| <<Process User Dataset Store Change>>
| RabbitMQ <2>
| Process a change in the User Dataset Store that has been published to
  RabbitMQ.

| Reconciliation
| ???
| ???

| Project Sync
| RabbitMQ <3>
| ???
|===

.General Q & A
--
====
* [x] What if the communication between the service and the import plugin was
      handled via a RabbitMQ queue?
** This adds too much complication to the design.  If we had a stream management
   platform such as Apache Spark testing and debugging the intermediate queues
   would be difficult.
* [ ] How do we hide endpoints from the public API?
* [x] How are statuses displayed to the client/user?  We have multiple statuses
      for a single dataset, and it might be confusing to show them separately?
** Yes, the statuses will be separate as described in the misc notes below.
* [ ] Service will have to check the soft-delete flag before performing any
      actions?
* [ ] Installers: what are the inputs and outputs?  Do we just post the dataset
      at the installer as a bulk request and let it rip from there?
====
--

==== List User Datasets

. Client makes a `GET` request with the following optional query parameters:
.. project_id=<string>
.. shared_only=<boolean>
.. owned_only=<boolean>
. Service queries the postgres database for a listing of relevant results
. Service returns a listing of relevant results, each of which will include:
.. user dataset id
.. status(es)?


==== Get User Dataset Info

. Client makes a `GET` request for the target user dataset.
. Service queries postgres database for information about the target dataset?
. Service returns information about the target dataset which will include:
.. user dataset id
.. offered shares?
.. status(es)?


==== Prepare User Dataset Upload

.Q & A
--
====
* [ ] What metadata is needed from the client about the dataset to be uploaded?
====
--

. Client makes a `POST` request with metadata about the dataset to be uploaded.
. Service sanity checks the metadata
. Service puts the metadata into a timed memory cache
. Service generates a dataset ID
. Service returns generated dataset ID


==== Upload User Dataset

. Client makes a multipart `POST` request with the dataset files to be uploaded.
. Service verifies the existence of the target user dataset ID in the memory
  cache.
. Service updates the status of the dataset upload to "uploading" (or similar)
. Service writes metadata file to a temp store to await processing by the import
  handler.
.. What is this temp store?
. Service writes the dataset files to the temp store to await processing by the
  import handler.
. Service marks the status of the dataset upload as "awaiting import" (or
  similar)
. Service queues the dataset to be import processed


==== Update User Dataset Metadata

. Client makes a `PATCH` request to the user dataset containing the fields that
  should be updated.
. Service verifies the existence of the target user dataset
.. How?
. Service verifies ownership of the target user dataset
.. How?
. Service performs sanity checking on the metadata being changed.
.. Ensure only mutable fields are being changed
.. Ensure the data going into those mutable fields is the correct type
. Service writes the updated metadata to the User Dataset Store
. Service returns OK


==== Offer User Dataset Share

NOTE: Path: `/user-datasets/\{ud-id}/shares/\{user-id}`

. Client makes a `PUT` request to the above URL with a body containing an action
  of "grant" or "revoke".
. Service sanity checks PUT request body
. Service verifies the existence of the target user dataset
. Service verifies that the target user dataset is owned by the requesting user
. Service writes a share offer file containing the requested action to the User
  Dataset Store


==== Accept User Dataset Share

NOTE: Path: `/user-datasets/\{ud-id}/shares/\{user-id}`

. Client makes a PUT request to the above URL with a body containing an action
  of "accept" or "reject"
. Service sanity checks PUT request body.
. Service verifies the existence of the target user dataset
. Service verifies that the target user dataset has a share offer available with
  an offer action of "grant"
. Service writes a share receipt file containing the requested action to the
  User Dataset Store


==== Delete User Dataset

NOTE: Path: `/user-datasets/\{ud-id}`

. Client makes a `DELETE` request to the above service path.
. Service verifies the target user dataset exists
. Service verifies the requesting user owns the target user dataset
. Service creates a `deleted` flag file for the user dataset in the User Dataset
  Store


==== Process Upload

. Service downloads the relevant files from the temp file store and pipes them
  through to the import handler as a multipart `POST` request.
.. ? Should the old style of a separate prep request and submission request be
   kept?  Is that necessary anymore?  I don't remember why it was done as two
   separate requests in the first place.
. Import handler does whatever it needs to validate and/or transform the
  imported user dataset and returns an archive file containing the outputs to be
  pushed to the user dataset store.
. Service unpacks the archive returned by the import handler
. Service uploads the files from the archive as a new dataset to the User
  Dataset Store

==== Process User Dataset Store Change

. Determine the nature of the change ???
.. What are the possible changes that could happen?
... marked as deleted
... actually deleted?
... share granted
... share accepted
... share rejected
... share revoked
... initial upload
... meta changed
.. Compare the last modified timestamps in S3 to the timestamps in the postgres
   `sync_control` table.
. ???
. Update postgres?
. Queue changes to relevant application databases?


== Unorganized Notes

=== Submitting a User Dataset

. Client sends "prep" request with metadata about the dataset to be
  uploaded.
.. Service sanity checks the posted metadata to ensure that it at least _could
   be_ valid.
.. Service puts the metadata into an in-memory cache with a short, configurable
   expiration
.. Service generates a user dataset ID
.. Service returns a user dataset ID
. Client sends an upload request with the file or files comprising the user
  dataset.
.. Service pulls the metadata for the user dataset out of the in-memory cache.
.. Service submits the metadata and the uploaded files to an internal job queue.
.. Service returns a status indicating whether the import process has been
   started


=== [Internal] Processing an Import

When a worker thread becomes available to process an import, it will be pulled
from the queue and the following will be executed.

. Worker submits the metadata for the job to be processed to the import handler
  plugin.
.. Import handler does whatever it needs to do to prepare for processing a user
   dataset.
. Worker submits the files for the dataset to the import handler.
.. Import handler processes user dataset and produces a gzip bundle of the
   dataset state to be uploaded to the Dataset Store
. Worker unpacks dataset bundle
. Worker uploads dataset files to the Dataset Store
. Worker updates the status of the dataset to "imported" or similar

// TODO: make a flowchart of a single "event" going through the process

// Multiple import queues?  Import queue per importer? (maybe phase 2)

== Misc Notes

Notes and thoughts to be folded into the design doc above once resolved.

=== Statuses

What different statuses are there?::
* Upload status
* `userdataset` table status (appears to also be upload status?)
* Install status (per project) (this field will be omitted or empty until the
  import is completed successfully)
+
.Status representation idea?
[source, json]
----
{
  "statuses": {
    "import": "complete",
    "install": [
      {
        "projectID": "PlasmoDB",
        "status": "complete"
      }
    ]
  }
}
----

=== Misc Diagrams

.User Dataset Import Components
image:assets/ds-import-components.png[]

== Database Schemata

=== Internal PostgreSQL Database

==== `sync_control`

This table indicates the last modified timestamp for the various components that
comprise a user dataset.

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| shares_update_time
| TIMESTAMPTZ
| Timestamp of the most recent last_modified date from the user dataset share
  files.

| data_update_time
| TIMESTAMPTZ
| Timestamp of the most recent last_modified date from the user dataset data
  files.

| meta_update_time
| TIMESTAMPTZ
| Timestamp of the meta.json last_modified date for the user dataset.
|===

==== `owner_share`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| shared_with
| BIGINT
| User ID of the user the dataset was shared with

| status
| granted \| revoked
| Current status of the share
|===

==== `recipient_share`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| shared_with
| BIGINT
| User ID of the user the dataset was shared with

| status
| accepted \| rejected
| Current status of the share receipt
|===

==== `user_dataset_control`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| upload_status
| enum
| "awaiting-upload", "uploading", "awaiting-import", "importing", "imported", "failed"
|===

==== `user_datasets`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| type_name
| VARCHAR
|

| type_version
| VARCHAR
|

| user_id
| BIGINT
|

| is_deleted
| BOOLEAN
|

| status
|
| ???

|===

==== `user_dataset_files`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| file_name
| VARCHAR
|
|===

==== `user_dataset_projects`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| project_id
| VARCHAR
|
|===

==== `user_dataset_metadata`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| name
| VARCHAR
|

| summary
| VARCHAR
|

| description
| VARCHAR
|
|===

=== Application Database

==== `user_datasets`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| owner
| BIGINT
| Owner user ID

| type
| VARCHAR
|

| version
| VARCHAR
|

| creation_time
| TIMESTAMP
|

| is_deleted
| TINYINT(1)
| Soft delete flag.
|===

==== `user_dataset_install_messages`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| message_id
|
| ???

| install_type
|
| ???

| status
| enum
| "running", "complete", "failed", "ready-for-reinstall"

| message
| VARCHAR
| failure message?
|===

==== `user_dataset_visibility`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| user_id
| BIGINT
| ID of the share recipient user who should be able to see the user dataset.
|===

==== `user_dataset_projects`

[%header, cols="2m,1m,7"]
|===
| Column | Type | Comment

| user_dataset_id
| CHAR(32)
|

| project_id
| VARCHAR
|
|===